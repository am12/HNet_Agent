{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1073621",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5258df3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T09:19:48.720140Z",
     "iopub.status.busy": "2026-01-29T09:19:48.719955Z",
     "iopub.status.idle": "2026-01-29T09:19:49.994757Z",
     "shell.execute_reply": "2026-01-29T09:19:49.994131Z"
    },
    "papermill": {
     "duration": 1.278775,
     "end_time": "2026-01-29T09:19:49.995754",
     "exception": false,
     "start_time": "2026-01-29T09:19:48.716979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# H-Net Text Generation Tutorial\n",
    "# This notebook demonstrates how to use the H-Net model for text generation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 300       # resolution of figures when shown\n",
    "plt.rcParams[\"savefig.dpi\"] = 300      # resolution when saving with plt.savefig\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import sys\n",
    "from omegaconf import ListConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe66ae",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23af6fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T09:19:49.999469Z",
     "iopub.status.busy": "2026-01-29T09:19:49.999286Z",
     "iopub.status.idle": "2026-01-29T09:19:52.442642Z",
     "shell.execute_reply": "2026-01-29T09:19:52.441285Z"
    },
    "papermill": {
     "duration": 2.447942,
     "end_time": "2026-01-29T09:19:52.445533",
     "exception": true,
     "start_time": "2026-01-29T09:19:49.997591",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba SSM macOS: Running on Apple Silicon with MPS acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/tmp/Paper2Agent/HNet_Agent/hnet-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixer_seq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HNetForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_hnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     AttnConfig,\n\u001b[1;32m      4\u001b[0m     SSMConfig,\n\u001b[1;32m      5\u001b[0m     HNetConfig,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ByteTokenizer\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixer_seq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HNetForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/models/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_hnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AttnConfig,\n\u001b[1;32m      3\u001b[0m     SSMConfig,\n\u001b[1;32m      4\u001b[0m     HNetConfig,\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HNet\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixer_seq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HNetForCausalLM\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/models/hnet.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misotropic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Isotropic, IsotropicInferenceParams\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     RoutingModule,\n\u001b[1;32m     10\u001b[0m     ChunkLayer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     DeChunkState,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_optimization_params\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Block\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmha\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalMHA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SwiGLU\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/modules/block.py:41\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmamba2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mamba2\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmha\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalMHA\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SwiGLU\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMamba2Wrapper\u001b[39;00m(Mamba2):\n",
      "File \u001b[0;32m/private/tmp/Paper2Agent/HNet_Agent/repo/hnet/hnet/modules/mha.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     flash_attn_kvpacked_func,\n\u001b[1;32m      9\u001b[0m     flash_attn_qkvpacked_func,\n\u001b[1;32m     10\u001b[0m     flash_attn_varlen_kvpacked_func,\n\u001b[1;32m     11\u001b[0m     flash_attn_varlen_qkvpacked_func,\n\u001b[1;32m     12\u001b[0m     flash_attn_with_kvcache,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrotary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RotaryEmbedding\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFlashCausalSelfAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "from hnet.models.mixer_seq import HNetForCausalLM\n",
    "from hnet.models.config_hnet import (\n",
    "    AttnConfig,\n",
    "    SSMConfig,\n",
    "    HNetConfig,\n",
    ")\n",
    "from hnet.utils.tokenizers import ByteTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17627bb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_from_pretrained(model_path: str, model_config_path: str):\n",
    "    \"\"\"Load model from pretrained checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the model checkpoint (.pt file)\n",
    "        model_config_path: Path to the model configuration (.json file)\n",
    "\n",
    "    Returns:\n",
    "        Loaded HNetForCausalLM model\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(model_config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Create config objects\n",
    "    attn_cfg = AttnConfig(**config.pop(\"attn_cfg\"))\n",
    "    ssm_cfg = SSMConfig(**config.pop(\"ssm_cfg\"))\n",
    "    hnet_cfg = HNetConfig(**config, attn_cfg=attn_cfg, ssm_cfg=ssm_cfg)\n",
    "\n",
    "    # Create model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = HNetForCausalLM(hnet_cfg, device=device, dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "\n",
    "    # Load checkpoint\n",
    "    major, minor = map(int, torch.__version__.split('.')[:2])\n",
    "    if (major, minor) >= (2, 6):\n",
    "        with torch.serialization.safe_globals([ListConfig]):\n",
    "            state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    else:\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb4248",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"Generate text from the model, yielding tokens as they're generated.\n",
    "\n",
    "    Args:\n",
    "        model: HNetForCausalLM model\n",
    "        prompt: Input text prompt\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_p: Top-p sampling parameter\n",
    "\n",
    "    Yields:\n",
    "        Generated text token by token as strings\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    tokenizer = ByteTokenizer()\n",
    "\n",
    "    # Tokenize prompt\n",
    "    encoded = tokenizer.encode([prompt], add_bos=True)[0]\n",
    "    input_ids = torch.tensor(\n",
    "        encoded[\"input_ids\"], dtype=torch.long, device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    inference_cache = model.allocate_inference_cache(\n",
    "        1, input_ids.shape[1] + max_tokens, dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        mask = torch.ones(input_ids.shape, device=device, dtype=torch.bool)\n",
    "        output = model.forward(input_ids, mask=mask, inference_params=inference_cache)\n",
    "\n",
    "    logits = output.logits[0, -1, :] / temperature\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Apply top-p sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(\n",
    "                torch.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "            sorted_indices_to_remove[0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = -float(\"inf\")\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_idx:\n",
    "            break\n",
    "\n",
    "        current_token = next_token.unsqueeze(0)\n",
    "        yield current_token\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = model.step(current_token, inference_cache)\n",
    "\n",
    "        # Get logits and apply temperature\n",
    "        logits = output.logits[0, -1, :] / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b7436",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model_path = \"/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/data/hnet_2stage_L.pt\"\n",
    "config_path = \"/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/data/hnet_2stage_L.json\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = load_from_pretrained(model_path, config_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1270d53",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example 1: Generate text with default parameters\n",
    "prompt1 = \"Once upon a time\"\n",
    "max_tokens = 200\n",
    "temperature = 1.0\n",
    "top_p = 0.9\n",
    "\n",
    "print(f\"Prompt: {prompt1}\")\n",
    "print(f\"Parameters: max_tokens={max_tokens}, temperature={temperature}, top_p={top_p}\\n\")\n",
    "print(\"Generated text:\")\n",
    "print(prompt1, end=\"\")\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "buf = []\n",
    "\n",
    "for token in generate(model, prompt1, max_tokens=max_tokens, temperature=temperature, top_p=top_p):\n",
    "    buf.append(token)\n",
    "    \n",
    "    decoded = None\n",
    "    res = None\n",
    "    for j in range(1, min(len(buf), 4)):\n",
    "        try:\n",
    "            res = tokenizer.decode(buf[:j])\n",
    "            decoded = j\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if res is not None:\n",
    "        print(res, end=\"\", flush=True)\n",
    "        buf = buf[decoded:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0g9b5izwjrpv",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example 2: Generate with different temperature (more creative)\n",
    "prompt2 = \"The future of artificial intelligence\"\n",
    "max_tokens = 150\n",
    "temperature = 1.2\n",
    "top_p = 0.95\n",
    "\n",
    "print(f\"Prompt: {prompt2}\")\n",
    "print(f\"Parameters: max_tokens={max_tokens}, temperature={temperature}, top_p={top_p}\\n\")\n",
    "print(\"Generated text:\")\n",
    "print(prompt2, end=\"\")\n",
    "\n",
    "buf = []\n",
    "\n",
    "for token in generate(model, prompt2, max_tokens=max_tokens, temperature=temperature, top_p=top_p):\n",
    "    buf.append(token)\n",
    "    \n",
    "    decoded = None\n",
    "    res = None\n",
    "    for j in range(1, min(len(buf), 4)):\n",
    "        try:\n",
    "            res = tokenizer.decode(buf[:j])\n",
    "            decoded = j\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if res is not None:\n",
    "        print(res, end=\"\", flush=True)\n",
    "        buf = buf[decoded:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.297112,
   "end_time": "2026-01-29T09:19:53.169490",
   "environment_variables": {},
   "exception": true,
   "input_path": "/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/h_net_text_generation_script_execution.ipynb",
   "output_path": "/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/h_net_text_generation_script_execution_v1.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T09:19:47.872378",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}