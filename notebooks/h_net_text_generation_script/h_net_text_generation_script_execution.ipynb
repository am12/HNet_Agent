{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258df3b",
   "metadata": {},
   "outputs": [],
   "source": "# H-Net Text Generation Tutorial\n# This notebook demonstrates how to use the H-Net model for text generation\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.dpi\"] = 300       # resolution of figures when shown\nplt.rcParams[\"savefig.dpi\"] = 300      # resolution when saving with plt.savefig\n\nimport numpy as np\nimport json\nimport torch\nimport sys\nfrom omegaconf import ListConfig"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hnet.models.mixer_seq import HNetForCausalLM\n",
    "from hnet.models.config_hnet import (\n",
    "    AttnConfig,\n",
    "    SSMConfig,\n",
    "    HNetConfig,\n",
    ")\n",
    "from hnet.utils.tokenizers import ByteTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17627bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_pretrained(model_path: str, model_config_path: str):\n",
    "    \"\"\"Load model from pretrained checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the model checkpoint (.pt file)\n",
    "        model_config_path: Path to the model configuration (.json file)\n",
    "\n",
    "    Returns:\n",
    "        Loaded HNetForCausalLM model\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(model_config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Create config objects\n",
    "    attn_cfg = AttnConfig(**config.pop(\"attn_cfg\"))\n",
    "    ssm_cfg = SSMConfig(**config.pop(\"ssm_cfg\"))\n",
    "    hnet_cfg = HNetConfig(**config, attn_cfg=attn_cfg, ssm_cfg=ssm_cfg)\n",
    "\n",
    "    # Create model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = HNetForCausalLM(hnet_cfg, device=device, dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "\n",
    "    # Load checkpoint\n",
    "    major, minor = map(int, torch.__version__.split('.')[:2])\n",
    "    if (major, minor) >= (2, 6):\n",
    "        with torch.serialization.safe_globals([ListConfig]):\n",
    "            state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    else:\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"Generate text from the model, yielding tokens as they're generated.\n",
    "\n",
    "    Args:\n",
    "        model: HNetForCausalLM model\n",
    "        prompt: Input text prompt\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_p: Top-p sampling parameter\n",
    "\n",
    "    Yields:\n",
    "        Generated text token by token as strings\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    tokenizer = ByteTokenizer()\n",
    "\n",
    "    # Tokenize prompt\n",
    "    encoded = tokenizer.encode([prompt], add_bos=True)[0]\n",
    "    input_ids = torch.tensor(\n",
    "        encoded[\"input_ids\"], dtype=torch.long, device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    inference_cache = model.allocate_inference_cache(\n",
    "        1, input_ids.shape[1] + max_tokens, dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        mask = torch.ones(input_ids.shape, device=device, dtype=torch.bool)\n",
    "        output = model.forward(input_ids, mask=mask, inference_params=inference_cache)\n",
    "\n",
    "    logits = output.logits[0, -1, :] / temperature\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Apply top-p sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(\n",
    "                torch.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "            sorted_indices_to_remove[0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = -float(\"inf\")\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_idx:\n",
    "            break\n",
    "\n",
    "        current_token = next_token.unsqueeze(0)\n",
    "        yield current_token\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = model.step(current_token, inference_cache)\n",
    "\n",
    "        # Get logits and apply temperature\n",
    "        logits = output.logits[0, -1, :] / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b7436",
   "metadata": {},
   "outputs": [],
   "source": "# Load the pretrained model\nmodel_path = \"/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/data/hnet_2stage_L.pt\"\nconfig_path = \"/private/tmp/Paper2Agent/HNet_Agent/notebooks/h_net_text_generation_script/data/hnet_2stage_L.json\"\n\nprint(\"Loading model...\")\nmodel = load_from_pretrained(model_path, config_path)\nprint(\"Model loaded successfully!\")\nprint(f\"Model device: {next(model.parameters()).device}\")\nprint(f\"Model dtype: {next(model.parameters()).dtype}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1270d53",
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Generate text with default parameters\nprompt1 = \"Once upon a time\"\nmax_tokens = 200\ntemperature = 1.0\ntop_p = 0.9\n\nprint(f\"Prompt: {prompt1}\")\nprint(f\"Parameters: max_tokens={max_tokens}, temperature={temperature}, top_p={top_p}\\n\")\nprint(\"Generated text:\")\nprint(prompt1, end=\"\")\n\ntokenizer = ByteTokenizer()\nbuf = []\n\nfor token in generate(model, prompt1, max_tokens=max_tokens, temperature=temperature, top_p=top_p):\n    buf.append(token)\n    \n    decoded = None\n    res = None\n    for j in range(1, min(len(buf), 4)):\n        try:\n            res = tokenizer.decode(buf[:j])\n            decoded = j\n        except:\n            pass\n    \n    if res is not None:\n        print(res, end=\"\", flush=True)\n        buf = buf[decoded:]\n\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "id": "0g9b5izwjrpv",
   "source": "# Example 2: Generate with different temperature (more creative)\nprompt2 = \"The future of artificial intelligence\"\nmax_tokens = 150\ntemperature = 1.2\ntop_p = 0.95\n\nprint(f\"Prompt: {prompt2}\")\nprint(f\"Parameters: max_tokens={max_tokens}, temperature={temperature}, top_p={top_p}\\n\")\nprint(\"Generated text:\")\nprint(prompt2, end=\"\")\n\nbuf = []\n\nfor token in generate(model, prompt2, max_tokens=max_tokens, temperature=temperature, top_p=top_p):\n    buf.append(token)\n    \n    decoded = None\n    res = None\n    for j in range(1, min(len(buf), 4)):\n        try:\n            res = tokenizer.decode(buf[:j])\n            decoded = j\n        except:\n            pass\n    \n    if res is not None:\n        print(res, end=\"\", flush=True)\n        buf = buf[decoded:]\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}